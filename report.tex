% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{biblatex}
\addbibresource{bibliography.bib}
\nocite{*}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Bitcoin sentiment analysis},
  pdfauthor={Ambroise Thibault, Faune Blanchard, Maxime Lorenzo},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Bitcoin sentiment analysis}
\author{Ambroise Thibault, Faune Blanchard, Maxime Lorenzo}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, interior hidden, breakable, frame hidden, enhanced, borderline west={3pt}{0pt}{shadecolor}, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{4}
\tableofcontents
}
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

This project asks a simple question: can the mood of the market help us
make better decisions when trading cryptocurrencies? We adapted a
research paper called Ghost in the Machine \textcite{leland_bybee_2025},
which showed that in traditional markets, sentiment often builds up
before a crash. That made us wonder if we could take similar ideas and
apply them to crypto, which is even more emotional and unpredictable.
The goal was to see if tracking sentiment could help us understand how
prices are going to move. Often, crypto naysayers say that
cryptocurrency prices rely on nothing but speculation. Therefore, trying
to understand the market's ``irrational'' response to news can help shed
light on this debate. Even though it doesn't resolve the core question
of the intrinsic value of bitcoin, it can show if investors in
cryptocurrency are more or less emotional than investors in ``classic''
assets.

We started by collecting sentiment data from places like news headlines.
The idea was to measure whether the overall tone on a given day was
positive, negative, or somewhere in between. For this, we used the
methodology presented in the paper, which we adapted to work with
Mistral AI (the Large Language Model (LLM) developed by french startup
Mistral), which is cheaper and easier to scale than GPT 3.5 (the LLM
used in the paper). Feeding the headlines to the LLM helped us develop a
time series of a sentiment ``score'' going from -1 to 1, which we can
then compare to bictoin returns.

-\textgreater{} finish intro when we have final results

\hypertarget{generating-sentiment-score}{%
\section{Generating sentiment score}\label{generating-sentiment-score}}

\hypertarget{presentation-of-the-database}{%
\subsection{Presentation of the
database}\label{presentation-of-the-database}}

We collected a dataset from GitHub entitled
\href{https://github.com/soheilrahsaz/cryptoNewsDataset}{Crypto News
Dataset}. It aggregates daily headlines from a variety of crypto news
websites. Each headline is labeled with the cryptocurrencies to which it
refers. They are also timestamped down to the second the news was posted
on the site. We also have the number of likes, dislikes, comments and
others, although these are often 0, as well as the link to the article.
In this project, we will only be using the raw headlines in order to
generate the scores that we need.

Here is a quick view of the pre processed dataset with our function
``import\_data'' in the ``prompts'' module. This function reads the csv
files from the GitHub repository linked above, then it removes unwanted
columns and formats the date so that we only have the day at which it
was posted. We can see that there can be multiple headlines in one day,
or no headlines. This will cause issues that we will explain later on in
the project.

The function also writes the prompts that we will later feed to the
Large Language Model in order to properly generate the scores, which we
will explain in the following section.

\hypertarget{generating-prompts}{%
\subsection{Generating prompts}\label{generating-prompts}}

For each headline, we generate a prompt which asks the AI its beliefs
about the evolution of Bitcoin returns based on this headline. The
prompt we decided to use is the exact prompt presented in
\textcite{leland_bybee_2025} . The exact prompt reads :

\begin{verbatim}
Here is a piece of news:
"%s"
Do you think this news will increase or decrease BTC?
Write your answer as: 
{increase/decrease/uncertain}:
{confidence (0-1)}:
{magnitude of increase/decrease (0-1)}:
{explanation (less than 25 words)}
\end{verbatim}

Where ``\%s'' is replaced by the headline. After editing this prompt, we
notice that removing part of this prompt changes the results drastically
(from ``increase'' to ``decrease'' for the same headline). Therefore, we
decided to keep the prompt from the paper as is, even though we will
only use the results for ``increase'' or ``decrease'' for this project.
This prompt has been adapted from traditional retail investor surveys or
surveys of CFOs that are used to produce a sentiment indicator. However,
these are costly and slow. As presented in the paper, using AI models to
replace human answers mimics the same results on a much bigger and
faster scale.

More information about the construction on this prompt can be found in
\textcite{leland_bybee_2025} .

Here, we use the Mistral LLM to generate prompt answers. The Mistral AI
API is a service that gives us access to powerful language models,
similar to ChatGPT or Claude. These models can read and interpret text,
making them useful for tasks like summarizing news or detecting the tone
of an article. In our project, we use the Mistral API to process
financial headlines or crypto-related news and ask the model the prompt
presented earlier. The model then gives us a structured answer,
including its level of confidence and a short explanation.

Running sentiment scoring at a large scale using the Mistral API isn't
just about sending a bunch of text to a model. It quickly becomes a
technical challenge. If you're processing thousands (or even millions)
of headlines, you need to manage API requests carefully. Language models
are slow compared to normal scripts, and APIs often have strict rate
limits. If you send too many requests too fast, you'll get throttled or
even temporarily blocked. That means you need to build in delays, handle
retries, and keep track of how many requests you're sending per second
or minute.

To make this efficient, the code has to handle asynchronous execution
and concurrency. Instead of sending requests one by one (which would
make the code very inefficient and take a long time to generate
answers), we wanted to send many in parallel---without breaking the API
rules. That means using tools like Python's asyncio, aiohttp, or httpx,
and writing logic that can pause, retry failed calls, and wait when
needed. WWe however also needed to track which headlines have already
been scored to handle failures cleanly. Scaling this up without crashing
the system or getting banned by the API meant taking additional security
measures in the code.

-\textgreater{} insert pseudo code of ambroise's final api pipeline

\hypertarget{scores-methodology}{%
\subsection{Scores methodology}\label{scores-methodology}}

After getting the scores from the LLM, we are left with categorical
values for each news, while there are multiple news per day. Therefore,
we decide to assign 1 if the LLM says the news will increase BTC, -1 if
it says it will decrease, and if it is uncertain we will ignore the
headline entirely (so as to not create noise). Then, we aggregate every
headline for one day, so that we get a clean series with one sentiment
score per day. To aggregate, we use the formula presented in the paper:

\[
F^\text{gpt}_t *(X^k_{t+h}) = \frac{\sum_{i \in \mathcal{A}_t} \mathbb{I}(\text{Increase}^k_i) - \mathbb{I}(\text{Decrease}^k_i)}{\sum_{i \in \mathcal{A}_t} \mathbb{I}(\text{Increase}^k_i) + \sum_{i \in \mathcal{A}_t} \mathbb{I}(\text{Decrease}^k_i)}
\]

The EAR, or Ex-Ante Residual, is meant to capture the part of sentiment
that can't be predicted based on past patterns. We do this by using a
simple model (like an AR(1): a linear regression where the explanatory
variable is simply the lagged target) to see what today's sentiment
should be, based on yesterday's. Whatever's left over (i.e.~the part the
AR(1) model didn't expect) is the EAR. It's the surprise in the mood of
the market.

This surprise is important because it often reflects emotional or
irrational shifts. In crypto especially, people can get overly excited
or scared in ways that don't line up with the data. By focusing on these
unexpected jumps in sentiment, we're trying to isolate the moments when
crowd psychology might be taking over---when prices could be getting
ahead of reality.

\hypertarget{comparison-with-btc-returns}{%
\section{Comparison with BTC
returns}\label{comparison-with-btc-returns}}

\hypertarget{extracting-btc-returns}{%
\subsection{Extracting BTC returns}\label{extracting-btc-returns}}

\hypertarget{comparing-with-scores}{%
\subsection{Comparing with scores}\label{comparing-with-scores}}

\hypertarget{initial-results}{%
\subsubsection{Initial results}\label{initial-results}}

\hypertarget{problems-with-our-model}{%
\subsubsection{Problems with our model}\label{problems-with-our-model}}

\hypertarget{interpretation}{%
\subsubsection{Interpretation}\label{interpretation}}


\printbibliography[title=Conclusion]


\end{document}
